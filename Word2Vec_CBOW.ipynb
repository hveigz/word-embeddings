{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: <br>\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling <br>\n",
    "https://gist.github.com/GavinXing/9954ea846072e115bb07d9758892382c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our toy dataset with a few sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.', 'Computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers.', 'As', 'they', 'evolve,', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data.', 'The', 'evolution', 'of', 'a', 'process', 'is', 'directed', 'by', 'a', 'pattern', 'of', 'rules', 'called', 'a', 'program.', 'People', 'create', 'programs', 'to', 'direct', 'processes.', 'In', 'effect,', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells.']\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the dataset and map each word to a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 words in the vocabulary\n",
      "\n",
      "{'computers.': 0, 'abstract': 1, 'process': 2, 'with': 3, 'of': 4, 'they': 5, 'evolution': 6, 'manipulate': 7, 'evolve,': 8, 'processes': 9, 'create': 10, 'other': 11, 'is': 12, 'In': 13, 'inhabit': 14, 'by': 15, 'directed': 16, 'spirits': 17, 'As': 18, 'We': 19, 'program.': 20, 'conjure': 21, 'pattern': 22, 'study': 23, 'spells.': 24, 'beings': 25, 'about': 26, 'things': 27, 'direct': 28, 'our': 29, 'process.': 30, 'to': 31, 'that': 32, 'processes.': 33, 'The': 34, 'rules': 35, 'Computational': 36, 'idea': 37, 'a': 38, 'programs': 39, 'are': 40, 'data.': 41, 'effect,': 42, 'the': 43, 'computer': 44, 'People': 45, 'called': 46, 'computational': 47, 'we': 48}\n"
     ]
    }
   ],
   "source": [
    "print(str(vocab_size) + \" words in the vocabulary\")\n",
    "print()\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the context - target observations to input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to transform input observations into vectors with corresponding vocabulary indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'to', 'study']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First observation\n",
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19,  40,  31,  23])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_context_vector(data[0][0], word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107f5ca50>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, context_size, embedding_size, vocab_size=None):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def embed(self, inputs):\n",
    "        embedded = self.embeddings(inputs)\n",
    "        return embedded\n",
    "                \n",
    "    def forward(self, inputs):        \n",
    "        embeds_sum = self.embed(inputs).sum(dim=0)\n",
    "        out = self.linear1(embeds_sum)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 500\n",
    "\n",
    "vocab_size=vocab_size\n",
    "context_size=2\n",
    "embedding_size=10\n",
    "\n",
    "random_seed = 77777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = CBOW(context_size=context_size, embedding_size=embedding_size, vocab_size=vocab_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 - Loss: 253.784637\n",
      "Train Epoch: 1 - Loss: 229.940887\n",
      "Train Epoch: 2 - Loss: 210.646942\n",
      "Train Epoch: 3 - Loss: 194.869553\n",
      "Train Epoch: 4 - Loss: 181.499847\n",
      "Train Epoch: 5 - Loss: 169.792404\n",
      "Train Epoch: 6 - Loss: 159.379303\n",
      "Train Epoch: 7 - Loss: 150.042801\n",
      "Train Epoch: 8 - Loss: 141.616898\n",
      "Train Epoch: 9 - Loss: 133.963028\n",
      "Train Epoch: 10 - Loss: 126.963890\n",
      "Train Epoch: 11 - Loss: 120.521767\n",
      "Train Epoch: 12 - Loss: 114.556618\n",
      "Train Epoch: 13 - Loss: 109.004013\n",
      "Train Epoch: 14 - Loss: 103.812393\n",
      "Train Epoch: 15 - Loss: 98.940247\n",
      "Train Epoch: 16 - Loss: 94.353813\n",
      "Train Epoch: 17 - Loss: 90.025375\n",
      "Train Epoch: 18 - Loss: 85.931755\n",
      "Train Epoch: 19 - Loss: 82.053535\n",
      "Train Epoch: 20 - Loss: 78.374191\n",
      "Train Epoch: 21 - Loss: 74.879715\n",
      "Train Epoch: 22 - Loss: 71.558121\n",
      "Train Epoch: 23 - Loss: 68.399033\n",
      "Train Epoch: 24 - Loss: 65.393440\n",
      "Train Epoch: 25 - Loss: 62.533272\n",
      "Train Epoch: 26 - Loss: 59.811218\n",
      "Train Epoch: 27 - Loss: 57.220417\n",
      "Train Epoch: 28 - Loss: 54.754356\n",
      "Train Epoch: 29 - Loss: 52.406826\n",
      "Train Epoch: 30 - Loss: 50.171741\n",
      "Train Epoch: 31 - Loss: 48.043282\n",
      "Train Epoch: 32 - Loss: 46.015888\n",
      "Train Epoch: 33 - Loss: 44.084164\n",
      "Train Epoch: 34 - Loss: 42.243095\n",
      "Train Epoch: 35 - Loss: 40.487919\n",
      "Train Epoch: 36 - Loss: 38.814259\n",
      "Train Epoch: 37 - Loss: 37.217995\n",
      "Train Epoch: 38 - Loss: 35.695358\n",
      "Train Epoch: 39 - Loss: 34.242867\n",
      "Train Epoch: 40 - Loss: 32.857262\n",
      "Train Epoch: 41 - Loss: 31.535542\n",
      "Train Epoch: 42 - Loss: 30.274921\n",
      "Train Epoch: 43 - Loss: 29.072805\n",
      "Train Epoch: 44 - Loss: 27.926758\n",
      "Train Epoch: 45 - Loss: 26.834467\n",
      "Train Epoch: 46 - Loss: 25.793770\n",
      "Train Epoch: 47 - Loss: 24.802584\n",
      "Train Epoch: 48 - Loss: 23.858906\n",
      "Train Epoch: 49 - Loss: 22.960806\n",
      "Train Epoch: 50 - Loss: 22.106396\n",
      "Train Epoch: 51 - Loss: 21.293840\n",
      "Train Epoch: 52 - Loss: 20.521301\n",
      "Train Epoch: 53 - Loss: 19.787014\n",
      "Train Epoch: 54 - Loss: 19.089193\n",
      "Train Epoch: 55 - Loss: 18.426119\n",
      "Train Epoch: 56 - Loss: 17.796087\n",
      "Train Epoch: 57 - Loss: 17.197433\n",
      "Train Epoch: 58 - Loss: 16.628544\n",
      "Train Epoch: 59 - Loss: 16.087843\n",
      "Train Epoch: 60 - Loss: 15.573816\n",
      "Train Epoch: 61 - Loss: 15.084999\n",
      "Train Epoch: 62 - Loss: 14.619987\n",
      "Train Epoch: 63 - Loss: 14.177449\n",
      "Train Epoch: 64 - Loss: 13.756108\n",
      "Train Epoch: 65 - Loss: 13.354753\n",
      "Train Epoch: 66 - Loss: 12.972239\n",
      "Train Epoch: 67 - Loss: 12.607481\n",
      "Train Epoch: 68 - Loss: 12.259462\n",
      "Train Epoch: 69 - Loss: 11.927219\n",
      "Train Epoch: 70 - Loss: 11.609843\n",
      "Train Epoch: 71 - Loss: 11.306491\n",
      "Train Epoch: 72 - Loss: 11.016367\n",
      "Train Epoch: 73 - Loss: 10.738722\n",
      "Train Epoch: 74 - Loss: 10.472856\n",
      "Train Epoch: 75 - Loss: 10.218121\n",
      "Train Epoch: 76 - Loss: 9.973886\n",
      "Train Epoch: 77 - Loss: 9.739591\n",
      "Train Epoch: 78 - Loss: 9.514694\n",
      "Train Epoch: 79 - Loss: 9.298685\n",
      "Train Epoch: 80 - Loss: 9.091104\n",
      "Train Epoch: 81 - Loss: 8.891498\n",
      "Train Epoch: 82 - Loss: 8.699458\n",
      "Train Epoch: 83 - Loss: 8.514591\n",
      "Train Epoch: 84 - Loss: 8.336535\n",
      "Train Epoch: 85 - Loss: 8.164947\n",
      "Train Epoch: 86 - Loss: 7.999505\n",
      "Train Epoch: 87 - Loss: 7.839910\n",
      "Train Epoch: 88 - Loss: 7.685876\n",
      "Train Epoch: 89 - Loss: 7.537138\n",
      "Train Epoch: 90 - Loss: 7.393445\n",
      "Train Epoch: 91 - Loss: 7.254557\n",
      "Train Epoch: 92 - Loss: 7.120254\n",
      "Train Epoch: 93 - Loss: 6.990330\n",
      "Train Epoch: 94 - Loss: 6.864581\n",
      "Train Epoch: 95 - Loss: 6.742824\n",
      "Train Epoch: 96 - Loss: 6.624881\n",
      "Train Epoch: 97 - Loss: 6.510587\n",
      "Train Epoch: 98 - Loss: 6.399785\n",
      "Train Epoch: 99 - Loss: 6.292322\n",
      "Train Epoch: 100 - Loss: 6.188061\n",
      "Train Epoch: 101 - Loss: 6.086871\n",
      "Train Epoch: 102 - Loss: 5.988619\n",
      "Train Epoch: 103 - Loss: 5.893189\n",
      "Train Epoch: 104 - Loss: 5.800469\n",
      "Train Epoch: 105 - Loss: 5.710348\n",
      "Train Epoch: 106 - Loss: 5.622723\n",
      "Train Epoch: 107 - Loss: 5.537501\n",
      "Train Epoch: 108 - Loss: 5.454584\n",
      "Train Epoch: 109 - Loss: 5.373888\n",
      "Train Epoch: 110 - Loss: 5.295326\n",
      "Train Epoch: 111 - Loss: 5.218822\n",
      "Train Epoch: 112 - Loss: 5.144298\n",
      "Train Epoch: 113 - Loss: 5.071680\n",
      "Train Epoch: 114 - Loss: 5.000902\n",
      "Train Epoch: 115 - Loss: 4.931896\n",
      "Train Epoch: 116 - Loss: 4.864603\n",
      "Train Epoch: 117 - Loss: 4.798959\n",
      "Train Epoch: 118 - Loss: 4.734907\n",
      "Train Epoch: 119 - Loss: 4.672395\n",
      "Train Epoch: 120 - Loss: 4.611366\n",
      "Train Epoch: 121 - Loss: 4.551775\n",
      "Train Epoch: 122 - Loss: 4.493571\n",
      "Train Epoch: 123 - Loss: 4.436709\n",
      "Train Epoch: 124 - Loss: 4.381145\n",
      "Train Epoch: 125 - Loss: 4.326838\n",
      "Train Epoch: 126 - Loss: 4.273744\n",
      "Train Epoch: 127 - Loss: 4.221828\n",
      "Train Epoch: 128 - Loss: 4.171053\n",
      "Train Epoch: 129 - Loss: 4.121382\n",
      "Train Epoch: 130 - Loss: 4.072779\n",
      "Train Epoch: 131 - Loss: 4.025215\n",
      "Train Epoch: 132 - Loss: 3.978656\n",
      "Train Epoch: 133 - Loss: 3.933072\n",
      "Train Epoch: 134 - Loss: 3.888435\n",
      "Train Epoch: 135 - Loss: 3.844717\n",
      "Train Epoch: 136 - Loss: 3.801888\n",
      "Train Epoch: 137 - Loss: 3.759924\n",
      "Train Epoch: 138 - Loss: 3.718802\n",
      "Train Epoch: 139 - Loss: 3.678495\n",
      "Train Epoch: 140 - Loss: 3.638982\n",
      "Train Epoch: 141 - Loss: 3.600240\n",
      "Train Epoch: 142 - Loss: 3.562248\n",
      "Train Epoch: 143 - Loss: 3.524985\n",
      "Train Epoch: 144 - Loss: 3.488429\n",
      "Train Epoch: 145 - Loss: 3.452563\n",
      "Train Epoch: 146 - Loss: 3.417368\n",
      "Train Epoch: 147 - Loss: 3.382827\n",
      "Train Epoch: 148 - Loss: 3.348920\n",
      "Train Epoch: 149 - Loss: 3.315631\n",
      "Train Epoch: 150 - Loss: 3.282947\n",
      "Train Epoch: 151 - Loss: 3.250849\n",
      "Train Epoch: 152 - Loss: 3.219323\n",
      "Train Epoch: 153 - Loss: 3.188355\n",
      "Train Epoch: 154 - Loss: 3.157929\n",
      "Train Epoch: 155 - Loss: 3.128032\n",
      "Train Epoch: 156 - Loss: 3.098653\n",
      "Train Epoch: 157 - Loss: 3.069777\n",
      "Train Epoch: 158 - Loss: 3.041392\n",
      "Train Epoch: 159 - Loss: 3.013486\n",
      "Train Epoch: 160 - Loss: 2.986047\n",
      "Train Epoch: 161 - Loss: 2.959066\n",
      "Train Epoch: 162 - Loss: 2.932530\n",
      "Train Epoch: 163 - Loss: 2.906429\n",
      "Train Epoch: 164 - Loss: 2.880751\n",
      "Train Epoch: 165 - Loss: 2.855488\n",
      "Train Epoch: 166 - Loss: 2.830632\n",
      "Train Epoch: 167 - Loss: 2.806170\n",
      "Train Epoch: 168 - Loss: 2.782096\n",
      "Train Epoch: 169 - Loss: 2.758399\n",
      "Train Epoch: 170 - Loss: 2.735071\n",
      "Train Epoch: 171 - Loss: 2.712105\n",
      "Train Epoch: 172 - Loss: 2.689492\n",
      "Train Epoch: 173 - Loss: 2.667224\n",
      "Train Epoch: 174 - Loss: 2.645293\n",
      "Train Epoch: 175 - Loss: 2.623693\n",
      "Train Epoch: 176 - Loss: 2.602417\n",
      "Train Epoch: 177 - Loss: 2.581457\n",
      "Train Epoch: 178 - Loss: 2.560805\n",
      "Train Epoch: 179 - Loss: 2.540457\n",
      "Train Epoch: 180 - Loss: 2.520404\n",
      "Train Epoch: 181 - Loss: 2.500643\n",
      "Train Epoch: 182 - Loss: 2.481166\n",
      "Train Epoch: 183 - Loss: 2.461968\n",
      "Train Epoch: 184 - Loss: 2.443041\n",
      "Train Epoch: 185 - Loss: 2.424383\n",
      "Train Epoch: 186 - Loss: 2.405985\n",
      "Train Epoch: 187 - Loss: 2.387844\n",
      "Train Epoch: 188 - Loss: 2.369954\n",
      "Train Epoch: 189 - Loss: 2.352310\n",
      "Train Epoch: 190 - Loss: 2.334908\n",
      "Train Epoch: 191 - Loss: 2.317741\n",
      "Train Epoch: 192 - Loss: 2.300807\n",
      "Train Epoch: 193 - Loss: 2.284100\n",
      "Train Epoch: 194 - Loss: 2.267617\n",
      "Train Epoch: 195 - Loss: 2.251351\n",
      "Train Epoch: 196 - Loss: 2.235300\n",
      "Train Epoch: 197 - Loss: 2.219459\n",
      "Train Epoch: 198 - Loss: 2.203825\n",
      "Train Epoch: 199 - Loss: 2.188394\n",
      "Train Epoch: 200 - Loss: 2.173161\n",
      "Train Epoch: 201 - Loss: 2.158123\n",
      "Train Epoch: 202 - Loss: 2.143278\n",
      "Train Epoch: 203 - Loss: 2.128619\n",
      "Train Epoch: 204 - Loss: 2.114145\n",
      "Train Epoch: 205 - Loss: 2.099853\n",
      "Train Epoch: 206 - Loss: 2.085738\n",
      "Train Epoch: 207 - Loss: 2.071798\n",
      "Train Epoch: 208 - Loss: 2.058029\n",
      "Train Epoch: 209 - Loss: 2.044429\n",
      "Train Epoch: 210 - Loss: 2.030994\n",
      "Train Epoch: 211 - Loss: 2.017722\n",
      "Train Epoch: 212 - Loss: 2.004610\n",
      "Train Epoch: 213 - Loss: 1.991655\n",
      "Train Epoch: 214 - Loss: 1.978854\n",
      "Train Epoch: 215 - Loss: 1.966205\n",
      "Train Epoch: 216 - Loss: 1.953704\n",
      "Train Epoch: 217 - Loss: 1.941351\n",
      "Train Epoch: 218 - Loss: 1.929141\n",
      "Train Epoch: 219 - Loss: 1.917073\n",
      "Train Epoch: 220 - Loss: 1.905143\n",
      "Train Epoch: 221 - Loss: 1.893352\n",
      "Train Epoch: 222 - Loss: 1.881694\n",
      "Train Epoch: 223 - Loss: 1.870169\n",
      "Train Epoch: 224 - Loss: 1.858774\n",
      "Train Epoch: 225 - Loss: 1.847507\n",
      "Train Epoch: 226 - Loss: 1.836367\n",
      "Train Epoch: 227 - Loss: 1.825350\n",
      "Train Epoch: 228 - Loss: 1.814455\n",
      "Train Epoch: 229 - Loss: 1.803681\n",
      "Train Epoch: 230 - Loss: 1.793024\n",
      "Train Epoch: 231 - Loss: 1.782484\n",
      "Train Epoch: 232 - Loss: 1.772058\n",
      "Train Epoch: 233 - Loss: 1.761745\n",
      "Train Epoch: 234 - Loss: 1.751542\n",
      "Train Epoch: 235 - Loss: 1.741449\n",
      "Train Epoch: 236 - Loss: 1.731462\n",
      "Train Epoch: 237 - Loss: 1.721582\n",
      "Train Epoch: 238 - Loss: 1.711806\n",
      "Train Epoch: 239 - Loss: 1.702132\n",
      "Train Epoch: 240 - Loss: 1.692559\n",
      "Train Epoch: 241 - Loss: 1.683086\n",
      "Train Epoch: 242 - Loss: 1.673711\n",
      "Train Epoch: 243 - Loss: 1.664433\n",
      "Train Epoch: 244 - Loss: 1.655249\n",
      "Train Epoch: 245 - Loss: 1.646159\n",
      "Train Epoch: 246 - Loss: 1.637161\n",
      "Train Epoch: 247 - Loss: 1.628254\n",
      "Train Epoch: 248 - Loss: 1.619438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 249 - Loss: 1.610709\n",
      "Train Epoch: 250 - Loss: 1.602067\n",
      "Train Epoch: 251 - Loss: 1.593511\n",
      "Train Epoch: 252 - Loss: 1.585040\n",
      "Train Epoch: 253 - Loss: 1.576652\n",
      "Train Epoch: 254 - Loss: 1.568346\n",
      "Train Epoch: 255 - Loss: 1.560121\n",
      "Train Epoch: 256 - Loss: 1.551976\n",
      "Train Epoch: 257 - Loss: 1.543910\n",
      "Train Epoch: 258 - Loss: 1.535921\n",
      "Train Epoch: 259 - Loss: 1.528009\n",
      "Train Epoch: 260 - Loss: 1.520172\n",
      "Train Epoch: 261 - Loss: 1.512410\n",
      "Train Epoch: 262 - Loss: 1.504721\n",
      "Train Epoch: 263 - Loss: 1.497103\n",
      "Train Epoch: 264 - Loss: 1.489559\n",
      "Train Epoch: 265 - Loss: 1.482083\n",
      "Train Epoch: 266 - Loss: 1.474678\n",
      "Train Epoch: 267 - Loss: 1.467341\n",
      "Train Epoch: 268 - Loss: 1.460072\n",
      "Train Epoch: 269 - Loss: 1.452869\n",
      "Train Epoch: 270 - Loss: 1.445732\n",
      "Train Epoch: 271 - Loss: 1.438660\n",
      "Train Epoch: 272 - Loss: 1.431652\n",
      "Train Epoch: 273 - Loss: 1.424708\n",
      "Train Epoch: 274 - Loss: 1.417826\n",
      "Train Epoch: 275 - Loss: 1.411005\n",
      "Train Epoch: 276 - Loss: 1.404245\n",
      "Train Epoch: 277 - Loss: 1.397546\n",
      "Train Epoch: 278 - Loss: 1.390905\n",
      "Train Epoch: 279 - Loss: 1.384323\n",
      "Train Epoch: 280 - Loss: 1.377799\n",
      "Train Epoch: 281 - Loss: 1.371332\n",
      "Train Epoch: 282 - Loss: 1.364921\n",
      "Train Epoch: 283 - Loss: 1.358565\n",
      "Train Epoch: 284 - Loss: 1.352265\n",
      "Train Epoch: 285 - Loss: 1.346018\n",
      "Train Epoch: 286 - Loss: 1.339825\n",
      "Train Epoch: 287 - Loss: 1.333685\n",
      "Train Epoch: 288 - Loss: 1.327597\n",
      "Train Epoch: 289 - Loss: 1.321561\n",
      "Train Epoch: 290 - Loss: 1.315575\n",
      "Train Epoch: 291 - Loss: 1.309640\n",
      "Train Epoch: 292 - Loss: 1.303755\n",
      "Train Epoch: 293 - Loss: 1.297919\n",
      "Train Epoch: 294 - Loss: 1.292131\n",
      "Train Epoch: 295 - Loss: 1.286391\n",
      "Train Epoch: 296 - Loss: 1.280698\n",
      "Train Epoch: 297 - Loss: 1.275052\n",
      "Train Epoch: 298 - Loss: 1.269452\n",
      "Train Epoch: 299 - Loss: 1.263898\n",
      "Train Epoch: 300 - Loss: 1.258389\n",
      "Train Epoch: 301 - Loss: 1.252924\n",
      "Train Epoch: 302 - Loss: 1.247504\n",
      "Train Epoch: 303 - Loss: 1.242127\n",
      "Train Epoch: 304 - Loss: 1.236793\n",
      "Train Epoch: 305 - Loss: 1.231501\n",
      "Train Epoch: 306 - Loss: 1.226252\n",
      "Train Epoch: 307 - Loss: 1.221044\n",
      "Train Epoch: 308 - Loss: 1.215878\n",
      "Train Epoch: 309 - Loss: 1.210752\n",
      "Train Epoch: 310 - Loss: 1.205665\n",
      "Train Epoch: 311 - Loss: 1.200619\n",
      "Train Epoch: 312 - Loss: 1.195612\n",
      "Train Epoch: 313 - Loss: 1.190643\n",
      "Train Epoch: 314 - Loss: 1.185713\n",
      "Train Epoch: 315 - Loss: 1.180821\n",
      "Train Epoch: 316 - Loss: 1.175967\n",
      "Train Epoch: 317 - Loss: 1.171149\n",
      "Train Epoch: 318 - Loss: 1.166368\n",
      "Train Epoch: 319 - Loss: 1.161624\n",
      "Train Epoch: 320 - Loss: 1.156915\n",
      "Train Epoch: 321 - Loss: 1.152241\n",
      "Train Epoch: 322 - Loss: 1.147603\n",
      "Train Epoch: 323 - Loss: 1.143000\n",
      "Train Epoch: 324 - Loss: 1.138431\n",
      "Train Epoch: 325 - Loss: 1.133895\n",
      "Train Epoch: 326 - Loss: 1.129394\n",
      "Train Epoch: 327 - Loss: 1.124925\n",
      "Train Epoch: 328 - Loss: 1.120490\n",
      "Train Epoch: 329 - Loss: 1.116087\n",
      "Train Epoch: 330 - Loss: 1.111716\n",
      "Train Epoch: 331 - Loss: 1.107377\n",
      "Train Epoch: 332 - Loss: 1.103069\n",
      "Train Epoch: 333 - Loss: 1.098793\n",
      "Train Epoch: 334 - Loss: 1.094547\n",
      "Train Epoch: 335 - Loss: 1.090332\n",
      "Train Epoch: 336 - Loss: 1.086147\n",
      "Train Epoch: 337 - Loss: 1.081992\n",
      "Train Epoch: 338 - Loss: 1.077866\n",
      "Train Epoch: 339 - Loss: 1.073770\n",
      "Train Epoch: 340 - Loss: 1.069703\n",
      "Train Epoch: 341 - Loss: 1.065664\n",
      "Train Epoch: 342 - Loss: 1.061654\n",
      "Train Epoch: 343 - Loss: 1.057672\n",
      "Train Epoch: 344 - Loss: 1.053717\n",
      "Train Epoch: 345 - Loss: 1.049790\n",
      "Train Epoch: 346 - Loss: 1.045891\n",
      "Train Epoch: 347 - Loss: 1.042018\n",
      "Train Epoch: 348 - Loss: 1.038172\n",
      "Train Epoch: 349 - Loss: 1.034352\n",
      "Train Epoch: 350 - Loss: 1.030559\n",
      "Train Epoch: 351 - Loss: 1.026791\n",
      "Train Epoch: 352 - Loss: 1.023049\n",
      "Train Epoch: 353 - Loss: 1.019332\n",
      "Train Epoch: 354 - Loss: 1.015641\n",
      "Train Epoch: 355 - Loss: 1.011975\n",
      "Train Epoch: 356 - Loss: 1.008333\n",
      "Train Epoch: 357 - Loss: 1.004715\n",
      "Train Epoch: 358 - Loss: 1.001122\n",
      "Train Epoch: 359 - Loss: 0.997552\n",
      "Train Epoch: 360 - Loss: 0.994006\n",
      "Train Epoch: 361 - Loss: 0.990484\n",
      "Train Epoch: 362 - Loss: 0.986985\n",
      "Train Epoch: 363 - Loss: 0.983509\n",
      "Train Epoch: 364 - Loss: 0.980056\n",
      "Train Epoch: 365 - Loss: 0.976625\n",
      "Train Epoch: 366 - Loss: 0.973217\n",
      "Train Epoch: 367 - Loss: 0.969830\n",
      "Train Epoch: 368 - Loss: 0.966466\n",
      "Train Epoch: 369 - Loss: 0.963123\n",
      "Train Epoch: 370 - Loss: 0.959802\n",
      "Train Epoch: 371 - Loss: 0.956502\n",
      "Train Epoch: 372 - Loss: 0.953224\n",
      "Train Epoch: 373 - Loss: 0.949966\n",
      "Train Epoch: 374 - Loss: 0.946729\n",
      "Train Epoch: 375 - Loss: 0.943513\n",
      "Train Epoch: 376 - Loss: 0.940317\n",
      "Train Epoch: 377 - Loss: 0.937140\n",
      "Train Epoch: 378 - Loss: 0.933985\n",
      "Train Epoch: 379 - Loss: 0.930849\n",
      "Train Epoch: 380 - Loss: 0.927732\n",
      "Train Epoch: 381 - Loss: 0.924635\n",
      "Train Epoch: 382 - Loss: 0.921557\n",
      "Train Epoch: 383 - Loss: 0.918499\n",
      "Train Epoch: 384 - Loss: 0.915459\n",
      "Train Epoch: 385 - Loss: 0.912438\n",
      "Train Epoch: 386 - Loss: 0.909435\n",
      "Train Epoch: 387 - Loss: 0.906451\n",
      "Train Epoch: 388 - Loss: 0.903486\n",
      "Train Epoch: 389 - Loss: 0.900538\n",
      "Train Epoch: 390 - Loss: 0.897608\n",
      "Train Epoch: 391 - Loss: 0.894696\n",
      "Train Epoch: 392 - Loss: 0.891801\n",
      "Train Epoch: 393 - Loss: 0.888924\n",
      "Train Epoch: 394 - Loss: 0.886065\n",
      "Train Epoch: 395 - Loss: 0.883222\n",
      "Train Epoch: 396 - Loss: 0.880397\n",
      "Train Epoch: 397 - Loss: 0.877588\n",
      "Train Epoch: 398 - Loss: 0.874796\n",
      "Train Epoch: 399 - Loss: 0.872021\n",
      "Train Epoch: 400 - Loss: 0.869262\n",
      "Train Epoch: 401 - Loss: 0.866519\n",
      "Train Epoch: 402 - Loss: 0.863792\n",
      "Train Epoch: 403 - Loss: 0.861082\n",
      "Train Epoch: 404 - Loss: 0.858387\n",
      "Train Epoch: 405 - Loss: 0.855708\n",
      "Train Epoch: 406 - Loss: 0.853044\n",
      "Train Epoch: 407 - Loss: 0.850397\n",
      "Train Epoch: 408 - Loss: 0.847765\n",
      "Train Epoch: 409 - Loss: 0.845147\n",
      "Train Epoch: 410 - Loss: 0.842545\n",
      "Train Epoch: 411 - Loss: 0.839957\n",
      "Train Epoch: 412 - Loss: 0.837385\n",
      "Train Epoch: 413 - Loss: 0.834827\n",
      "Train Epoch: 414 - Loss: 0.832284\n",
      "Train Epoch: 415 - Loss: 0.829755\n",
      "Train Epoch: 416 - Loss: 0.827240\n",
      "Train Epoch: 417 - Loss: 0.824740\n",
      "Train Epoch: 418 - Loss: 0.822254\n",
      "Train Epoch: 419 - Loss: 0.819782\n",
      "Train Epoch: 420 - Loss: 0.817323\n",
      "Train Epoch: 421 - Loss: 0.814879\n",
      "Train Epoch: 422 - Loss: 0.812448\n",
      "Train Epoch: 423 - Loss: 0.810030\n",
      "Train Epoch: 424 - Loss: 0.807626\n",
      "Train Epoch: 425 - Loss: 0.805236\n",
      "Train Epoch: 426 - Loss: 0.802858\n",
      "Train Epoch: 427 - Loss: 0.800494\n",
      "Train Epoch: 428 - Loss: 0.798142\n",
      "Train Epoch: 429 - Loss: 0.795804\n",
      "Train Epoch: 430 - Loss: 0.793478\n",
      "Train Epoch: 431 - Loss: 0.791165\n",
      "Train Epoch: 432 - Loss: 0.788865\n",
      "Train Epoch: 433 - Loss: 0.786577\n",
      "Train Epoch: 434 - Loss: 0.784302\n",
      "Train Epoch: 435 - Loss: 0.782038\n",
      "Train Epoch: 436 - Loss: 0.779787\n",
      "Train Epoch: 437 - Loss: 0.777548\n",
      "Train Epoch: 438 - Loss: 0.775321\n",
      "Train Epoch: 439 - Loss: 0.773106\n",
      "Train Epoch: 440 - Loss: 0.770903\n",
      "Train Epoch: 441 - Loss: 0.768712\n",
      "Train Epoch: 442 - Loss: 0.766531\n",
      "Train Epoch: 443 - Loss: 0.764363\n",
      "Train Epoch: 444 - Loss: 0.762206\n",
      "Train Epoch: 445 - Loss: 0.760061\n",
      "Train Epoch: 446 - Loss: 0.757926\n",
      "Train Epoch: 447 - Loss: 0.755803\n",
      "Train Epoch: 448 - Loss: 0.753692\n",
      "Train Epoch: 449 - Loss: 0.751591\n",
      "Train Epoch: 450 - Loss: 0.749501\n",
      "Train Epoch: 451 - Loss: 0.747422\n",
      "Train Epoch: 452 - Loss: 0.745353\n",
      "Train Epoch: 453 - Loss: 0.743295\n",
      "Train Epoch: 454 - Loss: 0.741248\n",
      "Train Epoch: 455 - Loss: 0.739212\n",
      "Train Epoch: 456 - Loss: 0.737185\n",
      "Train Epoch: 457 - Loss: 0.735170\n",
      "Train Epoch: 458 - Loss: 0.733164\n",
      "Train Epoch: 459 - Loss: 0.731169\n",
      "Train Epoch: 460 - Loss: 0.729184\n",
      "Train Epoch: 461 - Loss: 0.727209\n",
      "Train Epoch: 462 - Loss: 0.725244\n",
      "Train Epoch: 463 - Loss: 0.723288\n",
      "Train Epoch: 464 - Loss: 0.721343\n",
      "Train Epoch: 465 - Loss: 0.719407\n",
      "Train Epoch: 466 - Loss: 0.717482\n",
      "Train Epoch: 467 - Loss: 0.715566\n",
      "Train Epoch: 468 - Loss: 0.713659\n",
      "Train Epoch: 469 - Loss: 0.711762\n",
      "Train Epoch: 470 - Loss: 0.709874\n",
      "Train Epoch: 471 - Loss: 0.707996\n",
      "Train Epoch: 472 - Loss: 0.706127\n",
      "Train Epoch: 473 - Loss: 0.704267\n",
      "Train Epoch: 474 - Loss: 0.702416\n",
      "Train Epoch: 475 - Loss: 0.700575\n",
      "Train Epoch: 476 - Loss: 0.698742\n",
      "Train Epoch: 477 - Loss: 0.696918\n",
      "Train Epoch: 478 - Loss: 0.695104\n",
      "Train Epoch: 479 - Loss: 0.693298\n",
      "Train Epoch: 480 - Loss: 0.691501\n",
      "Train Epoch: 481 - Loss: 0.689712\n",
      "Train Epoch: 482 - Loss: 0.687933\n",
      "Train Epoch: 483 - Loss: 0.686161\n",
      "Train Epoch: 484 - Loss: 0.684399\n",
      "Train Epoch: 485 - Loss: 0.682645\n",
      "Train Epoch: 486 - Loss: 0.680899\n",
      "Train Epoch: 487 - Loss: 0.679161\n",
      "Train Epoch: 488 - Loss: 0.677432\n",
      "Train Epoch: 489 - Loss: 0.675711\n",
      "Train Epoch: 490 - Loss: 0.673998\n",
      "Train Epoch: 491 - Loss: 0.672294\n",
      "Train Epoch: 492 - Loss: 0.670597\n",
      "Train Epoch: 493 - Loss: 0.668909\n",
      "Train Epoch: 494 - Loss: 0.667228\n",
      "Train Epoch: 495 - Loss: 0.665555\n",
      "Train Epoch: 496 - Loss: 0.663890\n",
      "Train Epoch: 497 - Loss: 0.662233\n",
      "Train Epoch: 498 - Loss: 0.660584\n",
      "Train Epoch: 499 - Loss: 0.658943\n",
      "Train Epoch: 500 - Loss: 0.657308\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs+1):\n",
    "    total_loss = 0\n",
    "    for context, target in data:\n",
    "        context_var = make_context_vector(context, word_to_ix)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        probs = model(context_var).view(1,-1)\n",
    "        loss = loss_func(probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "        \n",
    "    print('Train Epoch: {} - Loss: {:.6f}'.format(\n",
    "                epoch, total_loss.item() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(data, i):\n",
    "    \n",
    "    v = make_context_vector(data[i][0], word_to_ix)\n",
    "    #print(v)\n",
    "    \n",
    "    output=model(v)    \n",
    "    #print(output)\n",
    "    \n",
    "    _, predicted = torch.max(output, 0)\n",
    "    \n",
    "    print(\"Context: \"+str(data[i][0]))\n",
    "    print(\"Output for the word with highest likelihood: \"+str(_.item()))\n",
    "    print(\"Predicted word: \"+str(ix_to_word[predicted.item()]))\n",
    "    print(\"True word: \"+str(data[i][1]))\n",
    "    print()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['We', 'are', 'to', 'study']\n",
      "Output for the word with highest likelihood: 12.126791000366211\n",
      "Predicted word: about\n",
      "True word: about\n",
      "\n",
      "Context: ['are', 'about', 'study', 'the']\n",
      "Output for the word with highest likelihood: 12.91408634185791\n",
      "Predicted word: to\n",
      "True word: to\n",
      "\n",
      "Context: ['about', 'to', 'the', 'idea']\n",
      "Output for the word with highest likelihood: 10.502835273742676\n",
      "Predicted word: study\n",
      "True word: study\n",
      "\n",
      "Context: ['to', 'study', 'idea', 'of']\n",
      "Output for the word with highest likelihood: 12.92435359954834\n",
      "Predicted word: the\n",
      "True word: the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    get_prediction(data, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check embedding vectors for the first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'to', 'study']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>We</th>\n",
       "      <td>0.126473</td>\n",
       "      <td>-0.678771</td>\n",
       "      <td>0.486892</td>\n",
       "      <td>-1.115773</td>\n",
       "      <td>1.589510</td>\n",
       "      <td>-0.743425</td>\n",
       "      <td>1.538858</td>\n",
       "      <td>1.566961</td>\n",
       "      <td>0.499367</td>\n",
       "      <td>-0.511873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>-1.034676</td>\n",
       "      <td>-0.252601</td>\n",
       "      <td>-1.139089</td>\n",
       "      <td>-0.681244</td>\n",
       "      <td>0.096220</td>\n",
       "      <td>-0.094143</td>\n",
       "      <td>1.211157</td>\n",
       "      <td>2.939287</td>\n",
       "      <td>-0.892874</td>\n",
       "      <td>0.790775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-2.353026</td>\n",
       "      <td>0.742168</td>\n",
       "      <td>-0.135938</td>\n",
       "      <td>1.189165</td>\n",
       "      <td>0.391911</td>\n",
       "      <td>0.499691</td>\n",
       "      <td>1.386475</td>\n",
       "      <td>-1.273110</td>\n",
       "      <td>-0.858032</td>\n",
       "      <td>0.206108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <td>0.415411</td>\n",
       "      <td>-1.001889</td>\n",
       "      <td>0.987945</td>\n",
       "      <td>0.937073</td>\n",
       "      <td>0.712470</td>\n",
       "      <td>0.521906</td>\n",
       "      <td>-1.272960</td>\n",
       "      <td>0.491454</td>\n",
       "      <td>-0.371379</td>\n",
       "      <td>-1.117039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "We     0.126473 -0.678771  0.486892 -1.115773  1.589510 -0.743425  1.538858   \n",
       "are   -1.034676 -0.252601 -1.139089 -0.681244  0.096220 -0.094143  1.211157   \n",
       "to    -2.353026  0.742168 -0.135938  1.189165  0.391911  0.499691  1.386475   \n",
       "study  0.415411 -1.001889  0.987945  0.937073  0.712470  0.521906 -1.272960   \n",
       "\n",
       "              7         8         9  \n",
       "We     1.566961  0.499367 -0.511873  \n",
       "are    2.939287 -0.892874  0.790775  \n",
       "to    -1.273110 -0.858032  0.206108  \n",
       "study  0.491454 -0.371379 -1.117039  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = make_context_vector(data[0][0], word_to_ix)\n",
    "embedded_vector = model.embed(vector).data.numpy()\n",
    "pd.DataFrame(embedded_vector, index=data[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
